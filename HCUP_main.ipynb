{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative and Positive Cohort**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psotive and Negative cohort. 1 to 1 ratio (50% positive and 50% negative)\n",
    "\n",
    "Basically, create a new file that contains patients who were diagnosed with diabetes and other specific \n",
    "diseases listed in diseases list.\n",
    "\n",
    "@Input file: indexed data excludes procedures. All patients in this file are diabetics <br/>\n",
    "@Output file:  a new file that contains patients who developed certain disease after 4 visits\n",
    "\n",
    "In addition, this file also keeps track of patients IDS who are positive, such that we can take\n",
    "negative patients by making sure the ID is not in the list of patients who tested positive\n",
    "<hr/>\n",
    "First, create two seperate files for positive and negative, then, take all positive patients and the same number from the negative patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = [\"72\"]\n",
    "diseases_count = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "patient_ids_with_positive_cohort = []\n",
    "output_file = 'HCUP_indexed_positive_d_311.csv'\n",
    "\n",
    "csv_positive_cohort = open(output_file, mode='w')\n",
    "\n",
    "file = 'HCUP_indexed_ex_procedures.csv'\n",
    "with open(file) as csv_file:\n",
    "    for line in csv_file:    \n",
    "        row = line.split(',')\n",
    "\n",
    "        for x in range(0, len(row)):\n",
    "            \n",
    "            if row[x] == '3':\n",
    "                i = x\n",
    "                data = row[i:] \n",
    "                # print(data)\n",
    "                c = 0\n",
    "                for i in range(0, len(data)):\n",
    "                    if '#' in data[i]:\n",
    "                        c+=1\n",
    "                    if c == 2: # 2 visits\n",
    "                        data_2 = data[:i]\n",
    "                        data_3 = data[i:]\n",
    "                        for j in range(len(diseases)):\n",
    "                            for x in data_3:\n",
    "                                \n",
    "                                if '#' in x: x = x.split('#')[0]\n",
    "                                \n",
    "                                if diseases[j] == x:\n",
    "                                    \n",
    "                                    flag = False\n",
    "                                    for y in data_2:\n",
    "                                        if diseases[j] == y:\n",
    "                                            flag = True\n",
    "                                            break\n",
    "                                    if not flag:\n",
    "                                        diseases_count[j] += 1\n",
    "                                        csv_positive_cohort.write(line) \n",
    "                                        patient_id = line.split('|')[0]\n",
    "                                        patient_ids_with_positive_cohort.append(patient_id) # to keep track\n",
    "                                        break        \n",
    "                                    \n",
    "                        break\n",
    "                break\n",
    "\n",
    "        \n",
    "csv_positive_cohort.close()\n",
    "print(diseases_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a dataset that contains all positive patients. \n",
    "Lets create a file with patients/diabetics who do not have specific disease (negative/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'HCUP_indexed_negative_d_311.csv'\n",
    "negative_cohort_file = open(out_file, mode='w')\n",
    "\n",
    "file = 'HCUP_indexed_ex_procedures.csv'\n",
    "\n",
    "with open(file, mode='r') as input_file:\n",
    "    for row in input_file:\n",
    "        patient_id = row.split('|')[0]\n",
    "        \n",
    "        # if patient id is not in the list of positive, then negative\n",
    "        if patient_id not in patient_ids_with_positive_cohort:\n",
    "            visits = row.replace('\\n','').split('|')[2].split('#')\n",
    "            if len(visits) >= 3:\n",
    "                negative_cohort_file.write(row)\n",
    "\n",
    "negative_cohort_file.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and count how many negative and positive patients there are \n",
    "take all positive patients and same number of patients from negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data-set\n",
    "file = 'HCUP_indexed_positive_d_311.csv'\n",
    "counter = 0\n",
    "with open(file) as test_data:\n",
    "    for row in test_data:\n",
    "        counter+=1\n",
    "print(counter)\n",
    "\n",
    "# Test data-set\n",
    "file = 'HCUP_indexed_negative_d_311.csv'\n",
    "counter = 0\n",
    "with open(file) as test_data:\n",
    "    for row in test_data:\n",
    "        #print(row)\n",
    "        counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two different datasets. Positive and negative patients\n",
    "\n",
    "Create a new file that contains 1 to 1 ratio - 50/50 balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 to 1 ratio - 50/50 - balanced data\n",
    "file = 'HCUP_D_311_1_to_1.csv'\n",
    "new_csv = open(file, mode='a')\n",
    "input_file = 'HCUP_indexed_positive_d_311.csv'\n",
    "\n",
    "counter = 0\n",
    "with open(input_file, mode='r') as data:\n",
    "    for row in data:\n",
    "        counter += 1\n",
    "        new_csv.write(row)\n",
    "new_csv.close()\n",
    "print('rows written', counter)\n",
    "n_pos = counter\n",
    "# first 21631 are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_pos)\n",
    "# 1 to 1 ratio - 50/50 - balanced data\n",
    "file = 'HCUP_D_311_1_to_1.csv'\n",
    "new_csv = open(file, mode='a')\n",
    "input_file = 'HCUP_indexed_negative_d_311.csv'\n",
    "\n",
    "counter = 0\n",
    "with open(input_file, mode='r') as data:\n",
    "    for row in data:\n",
    "        if counter == n_pos: break\n",
    "        counter += 1\n",
    "        new_csv.write(row)\n",
    "new_csv.close()\n",
    "# second 21631 are negative\n",
    "print('rows written', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and count total number - should be 21631*2 = 43262"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load med codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_d = {}\n",
    "file = 'disease_codes.csv'\n",
    "with open(file) as input_data:\n",
    "    for code in input_data:\n",
    "        index = code.split(',')[0][2:-1]\n",
    "        code = code.split(',')[1][1:-2]\n",
    "        \n",
    "        codes_d[code] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a look into the dict\n",
    "print(len(codes_d))\n",
    "for k, v in codes_d.items():\n",
    "    print(k, '-->', v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times a code occur per patient\n",
    "file = 'HCUP_D_311_1_to_1.csv'\n",
    "counter = 0\n",
    "\n",
    "code_count = {}\n",
    "\n",
    "patient_index = 1\n",
    "with open(file) as test_data:\n",
    "    for row in test_data:\n",
    "        visits = row.replace('\\n','').split('|')[2].split('#')\n",
    "        for k in range(0,len(visits)):\n",
    "            visit_codes = visits[k].split(',')[1:]\n",
    "            # print(visit_codes)\n",
    "            for v in visit_codes:\n",
    "                code_count[v] = code_count.get(v, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete those medical codes from dataset \n",
    "keys_to_delete = []\n",
    "counter = 0\n",
    "for k, v in code_count.items():\n",
    "     if v < 50 or v > 708290:\n",
    "        counter += 1\n",
    "        keys_to_delete.append(k)\n",
    "        \n",
    "print(counter)\n",
    "# delete those codes from data-set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in keys_to_delete:\n",
    "    print(i)\n",
    "    if i == '72':\n",
    "        print('-------------------------')\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new medical codes is a dict that exludes codes that we wanted to delete\n",
    "counter = 0\n",
    "new_med_codes = {}\n",
    "for k, v in codes_d.items():\n",
    "    if k in keys_to_delete:\n",
    "        continue\n",
    "    new_med_codes[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete procedures from new_med_codes\n",
    "med_codes_without_procedures = {}\n",
    "for k, v in new_med_codes.items():\n",
    "    if v.startswith(\"P_\"):\n",
    "        continue\n",
    "    med_codes_without_procedures[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new medical codes file - Code and its index\n",
    "import operator\n",
    "\n",
    "codes_file = open('disease_codes_v2.csv', 'w')\n",
    "\n",
    "for k, v in med_codes_without_procedures.items():\n",
    "    codes_file.write('(\\'' + v + '\\', ' + k + \")\\n\")\n",
    "codes_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new file that excludes medical codes to delete\n",
    "new_file = 'HCUP_D_311_1_to_1_v2.csv'\n",
    "\n",
    "new_csv = open(new_file, mode='w')\n",
    "\n",
    "file = 'HCUP_D_311_1_to_1.csv'\n",
    "\n",
    "with open(file) as input_file:\n",
    "    for line in input_file:\n",
    "        front = line.replace('\\n', '').split('|')[:2]\n",
    "        visits = line.replace('\\n','').split('|')[2].split('#')\n",
    "        new_visits = []\n",
    "        \n",
    "        for visit in visits:\n",
    "            link_and_age = visit.split(',')[:2]\n",
    "            med_codes = visit.split(',')[2:]\n",
    "            new_codes = []\n",
    "            \n",
    "            for med_code in med_codes:\n",
    "                # med_code is not '-99' and\n",
    "                 if med_code in med_codes_without_procedures:\n",
    "                    new_codes.append(str(med_code))\n",
    "            new_visits.append(','.join(link_and_age + new_codes))\n",
    "        new_csv.write('|'.join(front) + '|' + '#'.join(new_visits) + '\\n')\n",
    "new_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAKE AMENDMENT**\n",
    "\n",
    "For those patients who were tested positive, remove visits after that contain the certain complication we are looking for and after\n",
    "and for those who were tested negative, leave the length as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut positive patients such that we only have the row from 0 until we find the medical code of the required complication \n",
    "import sys\n",
    "file = 'HCUP_D_311_1_to_1_v2.csv'\n",
    "out_file = 'HCUP_D_311_1_to_1_v3.csv'\n",
    "new_csv = open(out_file, mode = 'a')\n",
    "counter = 0\n",
    "\n",
    "with open(file, mode='r') as csv_file:\n",
    "    for line in csv_file:\n",
    "        if counter < n_pos: # positive cohorts \n",
    "            counter += 1\n",
    "            front = line.replace('\\n', '').split('|')[:2]\n",
    "            visits = line.replace('\\n','').split('|')[2].split('#')\n",
    "\n",
    "            index = 0\n",
    "            for v in visits:\n",
    "                index+=1\n",
    "                if '72' in v[2:]:\n",
    "                    new_csv.write('|'.join(front) + '|' + '#'.join(visits[0:index-1]) + '\\n')\n",
    "                    break\n",
    "        else:\n",
    "            new_csv.write(line) \n",
    "new_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks avg number of visits per patients for positive and negative class to make sure we are not biased\n",
    "\n",
    "file = 'HCUP_D_311_1_to_1_v3.csv'\n",
    "\n",
    "avg_len_pos = 0\n",
    "avg_len_neg = 0\n",
    "\n",
    "counter = 0\n",
    "with open(file, mode='r') as csv_file:\n",
    "    for line in csv_file:\n",
    "        if counter < n_pos:\n",
    "            avg_len_pos += line.count(\"#\")\n",
    "        else:\n",
    "            avg_len_neg += line.count(\"#\")\n",
    "        counter += 1\n",
    "print(avg_len_pos / n_pos)\n",
    "print(avg_len_neg / n_pos)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates matrix where rows are visits containing all the diseases padded with zeros\n",
    "\n",
    "list_of_visits = []\n",
    "\n",
    "indexed_data = 'HCUP_D_311_1_to_1_v3.csv'\n",
    "with open(indexed_data) as input_data:\n",
    "    cnt_visits = 0\n",
    "    seen = set()\n",
    "    index = 0\n",
    "    for row in input_data:\n",
    "        visits = row.replace('\\n','').split('|')[2].split('#')\n",
    "        for visit in visits:\n",
    "            cnt_visits += 1\n",
    "            v = list(map(int, visit.split(',')[2:]))\n",
    "            v.sort()\n",
    "            v.extend([0] * (26 - len(v)))\n",
    "            v = tuple(v)\n",
    "            if v not in seen:\n",
    "                seen.add(v)\n",
    "                index += 1\n",
    "                \n",
    "    list_of_visits = [list(x) for x in seen]\n",
    "    print(list_of_visits[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medical codes\n",
    "codes_d = {}\n",
    "file = 'disease_codes_v2.csv'\n",
    "with open(file) as input_data:\n",
    "    index = 0\n",
    "    for code in input_data:\n",
    "        code = code.split(',')[1][1:-2]\n",
    "        codes_d[code] = index\n",
    "        index +=1 \n",
    "#print(codes_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Frequency Matrix\n",
    "# creates matrix where rows are visits and columns are diseases codes and values are frequnces for each disease in each visit\n",
    "freq_matrix = []\n",
    "for i in range(len(list_of_visits)):\n",
    "    freq_matrix.append([0] * 1150) # 1023\n",
    "\n",
    "for i in range(len(list_of_visits)):\n",
    "    for j in range(len(list_of_visits[0])):\n",
    "        if list_of_visits[i][j] != 0:\n",
    "            k = codes_d[str(list_of_visits[i][j])]\n",
    "            freq_matrix[i][k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs SVD on freq_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "visits_emb = svd.fit_transform(freq_matrix)\n",
    "print(len(visits_emb), len(visits_emb[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patients visits\n",
    "patients_visits = []\n",
    "\n",
    "indexed_data = 'HCUP_D_311_1_to_1_v3.csv'\n",
    "with open(indexed_data) as input_data:\n",
    "    for row in input_data:\n",
    "        v_codes = []\n",
    "        visits = row.replace('\\n','').split('|')[2].split('#')\n",
    "        for visit in visits:\n",
    "            v = list(map(int, visit.split(',')[2:]))\n",
    "            v.sort()\n",
    "            v.extend([0] * (26 - len(v)))\n",
    "            v_codes.append(list_of_visits.index(v))\n",
    "        patients_visits.append(v_codes)\n",
    "\n",
    "print(len(patients_visits), len(patients_visits[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change a matrix which rows are patients and col contains concatenated feature vectors of that patients visits\n",
    "rnn_input = []\n",
    "for i in range(len(patients_visits)):    # 43262\n",
    "    rnn_input.append([0] * 2500)\n",
    "    \n",
    "for i in range(len(patients_visits)):\n",
    "    for j in range(len(patients_visits[i])):\n",
    "        rnn_input[i][50 * j: 50 * (j + 1)] = visits_emb[patients_visits[i][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE SURE LENGTH OF ALL ROWS ARE 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write rnn_input to csv file\n",
    "import csv\n",
    "file = 'HCUP_rnn_input.csv'\n",
    "\n",
    "with open(file, \"w\", newline=\"\") as new_csv_file:\n",
    "    writer = csv.writer(new_csv_file)\n",
    "    for row in rnn_input:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peak into the new written file for testing purposes\n",
    "file = 'HCUP_rnn_input.csv'\n",
    "\n",
    "with open(file) as new_csv_file:\n",
    "    for line in new_csv_file:\n",
    "        data = line.split(',')\n",
    "        print((line))        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, Machine Learning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "RNN \n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries \n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "#from keras.models import Sequential\n",
    "#from tensorflow.keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation\n",
    "# from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "# plt.style.use('dark_background')\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D, GRU, Bidirectional, Input, TimeDistributed, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "rnn_input = []\n",
    "file = 'HCUP_rnn_input.csv'\n",
    "with open(file) as input_data:\n",
    "    for row in input_data:\n",
    "        row = row.replace('\\n','').split(',')\n",
    "        rnn_input.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dtype to float\n",
    "for r in range(0, len(rnn_input)):\n",
    "    for c in range(0, len(rnn_input[r])):\n",
    "        rnn_input[r][c] = float(rnn_input[r][c])\n",
    "print(type(rnn_input[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((rnn_input[0]))\n",
    "y = [1 if i < 135492 else 0 for i in range(135492*2)]\n",
    "print(len(y))\n",
    "print(len(rnn_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% training\n",
    "X = rnn_input\n",
    "# X = encoded\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print(len(X_train), 'training')\n",
    "print(len(X_test), 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search drop out rate and number of units/neurons\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "units = 128\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((50, 50), input_shape=(2500,)))\n",
    "# 1 sees the past \n",
    "# 2 sees the past and the future\n",
    "\n",
    "\n",
    "dropout = 0.2\n",
    "print('Number of hidden units: ', units, 'Dropout: ', dropout)\n",
    "model.add(Bidirectional(GRU(units, input_shape=input_shape))) # 64, $128$, 256, 512\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print('Training...')\n",
    "history = model.fit(\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    batch_size = 128, \n",
    "    epochs = 11,\n",
    "    validation_split = 0.1\n",
    ")\n",
    "\n",
    "print(model.evaluate(np.array(X_test), y_test), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'y', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1) #?\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'y', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 1 way LSTM\n",
    "# Random search drop out rate and number of units/neurons\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "units = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((50, 50), input_shape=(2500,)))\n",
    "\n",
    "# 1 sees the past \n",
    "# 2 sees the past and the future\n",
    "\n",
    "dropout = 1.9\n",
    "print('Number of hidden units: ', units, 'Dropout: ', dropout)\n",
    "model.add(LSTM(units, input_shape=input_shape)) # 64, $128$, 256, 512\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print('Training...')\n",
    "history = model.fit(\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    batch_size = 128, \n",
    "    epochs = 13,\n",
    "    validation_split = 0.1\n",
    ")\n",
    "\n",
    "print(model.evaluate(np.array(X_test), y_test), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the training and validation accuracy and loss at each epoch by using the history variable returned by the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'y', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1) #?\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'y', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "Classification Using SKlearn - Traditional ML Algorithms\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from time import time\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Random Forest\", \"MLP\"]\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=100),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "print(\"{0:20}{1:40}\\n-----------------------------------------------------\".\\\n",
    "      format(\"Classifier\", \"Accuracy\"))\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    \n",
    "    start_time = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end_time = time()\n",
    "    print(\"{0:20}{1:40}{2:40}\".format(name, str(score), (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
